{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VZ8GSNds3Vui",
        "outputId": "ef09b166-96ec-4716-fa7f-d8e617f6a274"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Get:1 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease [3,632 B]\n",
            "Get:2 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease [1,581 B]\n",
            "Hit:3 https://cli.github.com/packages stable InRelease\n",
            "Get:4 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ Packages [83.4 kB]\n",
            "Hit:5 http://archive.ubuntu.com/ubuntu jammy InRelease\n",
            "Get:6 http://security.ubuntu.com/ubuntu jammy-security InRelease [129 kB]\n",
            "Get:7 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  Packages [2,153 kB]\n",
            "Get:8 https://r2u.stat.illinois.edu/ubuntu jammy InRelease [6,555 B]\n",
            "Get:9 http://archive.ubuntu.com/ubuntu jammy-updates InRelease [128 kB]\n",
            "Get:10 https://r2u.stat.illinois.edu/ubuntu jammy/main all Packages [9,491 kB]\n",
            "Get:11 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease [18.1 kB]\n",
            "Hit:12 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy InRelease\n",
            "Get:13 http://security.ubuntu.com/ubuntu jammy-security/main amd64 Packages [3,535 kB]\n",
            "Get:14 http://archive.ubuntu.com/ubuntu jammy-backports InRelease [127 kB]\n",
            "Hit:15 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease\n",
            "Get:16 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 Packages [1,592 kB]\n",
            "Get:17 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy/main amd64 Packages [38.5 kB]\n",
            "Get:18 https://r2u.stat.illinois.edu/ubuntu jammy/main amd64 Packages [2,835 kB]\n",
            "Get:19 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 Packages [3,870 kB]\n",
            "Get:20 http://security.ubuntu.com/ubuntu jammy-security/universe amd64 Packages [1,286 kB]\n",
            "Get:21 http://archive.ubuntu.com/ubuntu jammy-updates/restricted amd64 Packages [6,222 kB]\n",
            "Get:22 http://security.ubuntu.com/ubuntu jammy-security/restricted amd64 Packages [6,008 kB]\n",
            "Fetched 37.5 MB in 3s (10.7 MB/s)\n",
            "Reading package lists...\n",
            "Reading package lists...\n",
            "Building dependency tree...\n",
            "Reading state information...\n",
            "tesseract-ocr is already the newest version (4.1.1-2.1build1).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 57 not upgraded.\n",
            "     â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 43.6/43.6 kB 1.9 MB/s eta 0:00:00\n",
            "     â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 67.7/67.7 kB 4.4 MB/s eta 0:00:00\n",
            "     â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 67.3/67.3 kB 6.2 MB/s eta 0:00:00\n",
            "   â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 60.0/60.0 kB 5.3 MB/s eta 0:00:00\n",
            "   â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 5.6/5.6 MB 90.8 MB/s eta 0:00:00\n",
            "   â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 21.4/21.4 MB 101.7 MB/s eta 0:00:00\n",
            "   â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 278.2/278.2 kB 27.2 MB/s eta 0:00:00\n",
            "   â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 2.0/2.0 MB 89.7 MB/s eta 0:00:00\n",
            "   â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 103.3/103.3 kB 10.7 MB/s eta 0:00:00\n",
            "   â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 17.4/17.4 MB 122.4 MB/s eta 0:00:00\n",
            "   â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 72.5/72.5 kB 6.3 MB/s eta 0:00:00\n",
            "   â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 132.3/132.3 kB 12.7 MB/s eta 0:00:00\n",
            "   â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 65.9/65.9 kB 6.3 MB/s eta 0:00:00\n",
            "   â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 208.0/208.0 kB 14.4 MB/s eta 0:00:00\n",
            "   â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 105.4/105.4 kB 9.2 MB/s eta 0:00:00\n",
            "   â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 71.6/71.6 kB 5.8 MB/s eta 0:00:00\n",
            "   â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 3.0/3.0 MB 110.0 MB/s eta 0:00:00\n",
            "   â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 517.7/517.7 kB 39.1 MB/s eta 0:00:00\n",
            "   â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 128.4/128.4 kB 12.0 MB/s eta 0:00:00\n",
            "   â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 4.4/4.4 MB 123.1 MB/s eta 0:00:00\n",
            "   â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 456.8/456.8 kB 38.7 MB/s eta 0:00:00\n",
            "   â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 46.0/46.0 kB 4.3 MB/s eta 0:00:00\n",
            "   â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 86.8/86.8 kB 8.8 MB/s eta 0:00:00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "W: Skipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\n",
            "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-adk 1.19.0 requires opentelemetry-api<=1.37.0,>=1.37.0, but you have opentelemetry-api 1.38.0 which is incompatible.\n",
            "google-adk 1.19.0 requires opentelemetry-sdk<=1.37.0,>=1.37.0, but you have opentelemetry-sdk 1.38.0 which is incompatible.\n",
            "opentelemetry-exporter-otlp-proto-http 1.37.0 requires opentelemetry-exporter-otlp-proto-common==1.37.0, but you have opentelemetry-exporter-otlp-proto-common 1.38.0 which is incompatible.\n",
            "opentelemetry-exporter-otlp-proto-http 1.37.0 requires opentelemetry-proto==1.37.0, but you have opentelemetry-proto 1.38.0 which is incompatible.\n",
            "opentelemetry-exporter-otlp-proto-http 1.37.0 requires opentelemetry-sdk~=1.37.0, but you have opentelemetry-sdk 1.38.0 which is incompatible.\n"
          ]
        }
      ],
      "source": [
        "%%bash\n",
        "# 1. Install System Dependencies for OCR\n",
        "sudo apt-get update -q\n",
        "sudo apt-get install -y tesseract-ocr -q\n",
        "\n",
        "# 2. Install Python Libraries\n",
        "pip install -q \\\n",
        "    pdfplumber \\\n",
        "    sentence-transformers \\\n",
        "    chromadb \\\n",
        "    openai \\\n",
        "    pytesseract \\\n",
        "    pillow \\\n",
        "    tiktoken\n",
        ""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import sys\n",
        "import uuid\n",
        "import shutil\n",
        "import pdfplumber\n",
        "import pytesseract\n",
        "import chromadb\n",
        "from PIL import Image\n",
        "from typing import List, Dict\n",
        "from google.colab import drive\n",
        "from google.colab import files\n",
        "from sentence_transformers import SentenceTransformer, CrossEncoder\n",
        "import openai\n",
        "\n",
        "# CONFIGURATION\n",
        "\n",
        "MOUNT_PATH = '/content/drive'\n",
        "PROJECT_DIR = '/content/drive/MyDrive/colab_rag_data'\n",
        "CHROMA_PATH = os.path.join(PROJECT_DIR, \"chroma_db\")\n",
        "EMBED_MODEL_NAME = \"all-MiniLM-L6-v2\" # Fast and effective\n",
        "CROSS_ENCODER_NAME = \"cross-encoder/ms-marco-MiniLM-L-6-v2\"\n",
        "\n",
        "\n",
        "# 1. SETUP & UTILITIES\n",
        "\n",
        "def setup_environment():\n",
        "    \"\"\"Mounts drive and creates project folders.\"\"\"\n",
        "    if not os.path.exists(MOUNT_PATH):\n",
        "        print(\"Mounting Google Drive...\")\n",
        "        drive.mount(MOUNT_PATH)\n",
        "\n",
        "    if not os.path.exists(PROJECT_DIR):\n",
        "        os.makedirs(PROJECT_DIR)\n",
        "        print(f\"Created project directory: {PROJECT_DIR}\")\n",
        "    else:\n",
        "        print(f\"Using project directory: {PROJECT_DIR}\")\n",
        "\n",
        "    # Get OpenAI Key safely\n",
        "    print(\"\\n--- API SETUP ---\")\n",
        "    api_key = input(\"Enter OpenAI API Key (press Enter to skip if testing retrieval only): \").strip()\n",
        "    if api_key:\n",
        "        os.environ[\"OPENAI_API_KEY\"] = api_key\n",
        "        openai.api_key = api_key\n",
        "        print(\"API Key set.\")\n",
        "    else:\n",
        "        print(\"No API Key provided. System will run in 'Retrieval Only' mode.\")\n",
        "    return api_key\n",
        "\n",
        "\n",
        "# 2. PDF PROCESSING & OCR\n",
        "\n",
        "def extract_text_from_pdf(pdf_path: str) -> List[Dict]:\n",
        "    \"\"\"Extracts text page-by-page. Uses OCR if text extraction fails.\"\"\"\n",
        "    print(f\"Processing: {os.path.basename(pdf_path)}...\")\n",
        "    extracted_pages = []\n",
        "\n",
        "    with pdfplumber.open(pdf_path) as pdf:\n",
        "        for i, page in enumerate(pdf.pages, start=1):\n",
        "            text = page.extract_text()\n",
        "\n",
        "            # If text is empty or very short, try OCR\n",
        "            if not text or len(text.strip()) < 50:\n",
        "                print(f\"  - Page {i}: Text empty, running OCR...\")\n",
        "                try:\n",
        "                    # Render page to image for OCR\n",
        "                    im = page.to_image(resolution=300).original\n",
        "                    text = pytesseract.image_to_string(im)\n",
        "                    is_ocr = True\n",
        "                except Exception as e:\n",
        "                    print(f\"    OCR Failed for page {i}: {e}\")\n",
        "                    text = \"\"\n",
        "                    is_ocr = False\n",
        "            else:\n",
        "                is_ocr = False\n",
        "\n",
        "            if text.strip():\n",
        "                extracted_pages.append({\n",
        "                    \"page\": i,\n",
        "                    \"text\": text,\n",
        "                    \"is_ocr\": is_ocr,\n",
        "                    \"source\": os.path.basename(pdf_path)\n",
        "                })\n",
        "\n",
        "    print(f\"Extraction complete. Found {len(extracted_pages)} usable pages.\")\n",
        "    return extracted_pages\n",
        "\n",
        "def chunk_text(pages: List[Dict], chunk_size=400, overlap=100) -> List[Dict]:\n",
        "    \"\"\"Splits pages into overlapping chunks.\"\"\"\n",
        "    chunks = []\n",
        "    for p in pages:\n",
        "        words = p['text'].split()\n",
        "        for i in range(0, len(words), chunk_size - overlap):\n",
        "            chunk_words = words[i:i + chunk_size]\n",
        "            chunk_text = \" \".join(chunk_words)\n",
        "\n",
        "            if len(chunk_text) > 50: # Skip tiny chunks\n",
        "                chunks.append({\n",
        "                    \"id\": str(uuid.uuid4()),\n",
        "                    \"text\": chunk_text,\n",
        "                    \"metadata\": {\n",
        "                        \"page\": p['page'],\n",
        "                        \"source\": p['source'],\n",
        "                        \"is_ocr\": p['is_ocr']\n",
        "                    }\n",
        "                })\n",
        "    print(f\"Chunking complete. Created {len(chunks)} chunks.\")\n",
        "    return chunks\n",
        "\n",
        "\n",
        "# 3. VECTOR DATABASE (CHROMA)\n",
        "\n",
        "class VectorStore:\n",
        "    def __init__(self, persist_path):\n",
        "        print(f\"Initializing Vector DB at {persist_path}...\")\n",
        "        self.client = chromadb.PersistentClient(path=persist_path)\n",
        "        self.collection = self.client.get_or_create_collection(name=\"pdf_knowledge_base\")\n",
        "        self.embedder = SentenceTransformer(EMBED_MODEL_NAME)\n",
        "        self.reranker = CrossEncoder(CROSS_ENCODER_NAME)\n",
        "\n",
        "    def add_documents(self, chunks: List[Dict]):\n",
        "        if not chunks: return\n",
        "\n",
        "        # Prepare data for Chroma\n",
        "        ids = [c['id'] for c in chunks]\n",
        "        docs = [c['text'] for c in chunks]\n",
        "        metas = [c['metadata'] for c in chunks]\n",
        "\n",
        "        print(\"Generating embeddings (this may take a moment)...\")\n",
        "        embeddings = self.embedder.encode(docs, show_progress_bar=True).tolist()\n",
        "\n",
        "        # Batch insert to avoid memory issues\n",
        "        batch_size = 100\n",
        "        for i in range(0, len(ids), batch_size):\n",
        "            self.collection.add(\n",
        "                ids=ids[i:i+batch_size],\n",
        "                documents=docs[i:i+batch_size],\n",
        "                metadatas=metas[i:i+batch_size],\n",
        "                embeddings=embeddings[i:i+batch_size]\n",
        "            )\n",
        "        print(\"Data indexed successfully.\")\n",
        "\n",
        "    def query(self, query_text: str, top_k=10, rerank_top_n=5):\n",
        "        # 1. Dense Retrieval\n",
        "        q_emb = self.embedder.encode([query_text]).tolist()\n",
        "        results = self.collection.query(\n",
        "            query_embeddings=q_emb,\n",
        "            n_results=top_k\n",
        "        )\n",
        "\n",
        "        # Flatten results\n",
        "        hits = []\n",
        "        if results['documents']:\n",
        "            for i, doc in enumerate(results['documents'][0]):\n",
        "                hits.append({\n",
        "                    \"text\": doc,\n",
        "                    \"metadata\": results['metadatas'][0][i],\n",
        "                    \"score\": results['distances'][0][i]\n",
        "                })\n",
        "\n",
        "        # 2. Re-ranking (Refining relevance)\n",
        "        if hits and rerank_top_n > 0:\n",
        "            pairs = [[query_text, hit['text']] for hit in hits]\n",
        "            scores = self.reranker.predict(pairs)\n",
        "            for i, hit in enumerate(hits):\n",
        "                hit['rerank_score'] = scores[i]\n",
        "\n",
        "            # Sort by re-rank score (higher is better)\n",
        "            hits = sorted(hits, key=lambda x: x['rerank_score'], reverse=True)\n",
        "            hits = hits[:rerank_top_n]\n",
        "\n",
        "        return hits\n",
        "\n",
        "\n",
        "# 4. LLM GENERATION\n",
        "\n",
        "def generate_answer(query: str, context_hits: List[Dict], api_key: str):\n",
        "    \"\"\"Constructs prompt and calls OpenAI.\"\"\"\n",
        "    if not api_key:\n",
        "        return \"No API Key provided. Skipping LLM generation.\"\n",
        "\n",
        "    # Build context string with citations\n",
        "    context_str = \"\"\n",
        "    for hit in context_hits:\n",
        "        meta = hit['metadata']\n",
        "        context_str += f\"[Source: {meta['source']}, Page: {meta['page']}]\\n{hit['text']}\\n\\n\"\n",
        "\n",
        "    prompt = f\"\"\"\n",
        "    You are a helpful research assistant. Use the provided context to answer the question.\n",
        "    If the answer is not in the context, state that you don't know.\n",
        "    Always cite the page numbers provided in the context blocks.\n",
        "\n",
        "    CONTEXT:\n",
        "    {context_str}\n",
        "\n",
        "    QUESTION:\n",
        "    {query}\n",
        "\n",
        "    ANSWER:\n",
        "    \"\"\"\n",
        "\n",
        "    try:\n",
        "        response = openai.ChatCompletion.create(\n",
        "            model=\"gpt-3.5-turbo\", # Switch to gpt-4 if you have access\n",
        "            messages=[{\"role\": \"user\", \"content\": prompt}],\n",
        "            temperature=0.0\n",
        "        )\n",
        "        return response.choices[0].message.content.strip()\n",
        "    except Exception as e:\n",
        "        return f\"Error calling OpenAI: {e}\"\n",
        "\n",
        "\n",
        "# 5. MAIN EXECUTION LOOP\n",
        "\n",
        "def main():\n",
        "    api_key = setup_environment()\n",
        "    db = VectorStore(CHROMA_PATH)\n",
        "\n",
        "    print(\"\\n--- FILE UPLOAD ---\")\n",
        "    print(\"1. Upload new PDF\")\n",
        "    print(\"2. Use existing index (Skip upload)\")\n",
        "    choice = input(\"Enter choice (1 or 2): \")\n",
        "\n",
        "    if choice == '1':\n",
        "        uploaded = files.upload()\n",
        "        if uploaded:\n",
        "            filename = list(uploaded.keys())[0]\n",
        "            # Process\n",
        "            raw_pages = extract_text_from_pdf(filename)\n",
        "            chunks = chunk_text(raw_pages)\n",
        "            db.add_documents(chunks)\n",
        "\n",
        "            # Optional: move file to Drive to keep it\n",
        "            dest_path = os.path.join(PROJECT_DIR, filename)\n",
        "            shutil.move(filename, dest_path)\n",
        "            print(f\"File saved to {dest_path}\")\n",
        "\n",
        "    print(\"\\n\" + \"=\"*40)\n",
        "    print(\"ğŸ¤– RAG CHATBOT READY\")\n",
        "    print(\"=\"*40)\n",
        "\n",
        "    while True:\n",
        "        query = input(\"\\nAsk a question (or type 'quit'): \")\n",
        "        if query.lower() in ['quit', 'exit']:\n",
        "            break\n",
        "\n",
        "        # Retrieve\n",
        "        print(\"ğŸ” Searching...\")\n",
        "        hits = db.query(query)\n",
        "\n",
        "        # Display Sources (for verification)\n",
        "        print(\"\\n--- Top Sources ---\")\n",
        "        for h in hits[:2]:\n",
        "            print(f\"[Page {h['metadata']['page']}] {h['text'][:150]}...\")\n",
        "\n",
        "        # Generate Answer\n",
        "        print(\"\\n--- Answer ---\")\n",
        "        if api_key:\n",
        "            answer = generate_answer(query, hits, api_key)\n",
        "            print(answer)\n",
        "        else:\n",
        "            print(\"No API Key. Review sources above.\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n",
        ""
      ],
      "metadata": {
        "id": "nDMnC41L3w1P"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}